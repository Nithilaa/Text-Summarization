{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text summarization_tfidf.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nithilaa/Text-Summarization/blob/master/text_summarization_tfidf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "584w4ou2psSU",
        "colab_type": "text"
      },
      "source": [
        "# **What is TFIDF Approach ?**\n",
        "\n",
        "\n",
        "TFIDF, short for term frequency–inverse document frequency, is a numeric measure that is use to score the importance of a word in a document based on how often did it appear in that document and a given collection of documents. \n",
        "\n",
        "The intuition behind this measure is : If a word appears frequently in a document, then it should be important and we should give that word a high score. \n",
        "\n",
        "But if a word appears in too many other documents, it’s probably not a unique identifier, therefore we should assign a lower score to that word.\n",
        "Formula for calculating tf and idf:\n",
        "\n",
        "\n",
        "    TF(w) = (Number of times term w appears in a document) / (Total number of terms in the document)\n",
        "\n",
        "\n",
        "    IDF(w) = log_e(Total number of documents / Number of documents with term w in it)\n",
        "\n",
        "\n",
        "Hence tfidf for a word can be calculated as:\n",
        "\n",
        "\n",
        "    TFIDF(w) = TF(w) * IDF(w)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CS3_jFjCo2DU",
        "colab_type": "text"
      },
      "source": [
        "# **Importing Libraries**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3l0Q8XtVjhyX",
        "colab_type": "text"
      },
      "source": [
        "The most important library for working with text in python is NLTK. \n",
        "\n",
        "It stands for Natural Language toolkit. It contains methods such as sent_tokenize, word_tokenize in the corpus package, which can split the text into sentences and words respectively. \n",
        "\n",
        "Stem package of NLTK contains methods for lemmatization, namely WordNetLemmatizer . \n",
        "\n",
        "Stopwords contains list of english stop words, which needs to be removed during preprocessing step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiAqTvaXQD-2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "e0c8e741-1379-4bd8-bd89-817d8d4bf27e"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dp2sto_HPyZ3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "5886941b-9d30-4925-b7db-cfa3f60c9fcf"
      },
      "source": [
        "import nltk\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "import operator\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize,word_tokenize\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "Stopwords = set(stopwords.words('english'))\n",
        "wordlemmatizer = WordNetLemmatizer()\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g796Pb2AQIPS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lemmatize_words(words):\n",
        "    lemmatized_words = []\n",
        "    for word in words:\n",
        "       lemmatized_words.append(wordlemmatizer.lemmatize(word))\n",
        "    return lemmatized_words\n",
        "\n",
        "\n",
        "def stem_words(words):\n",
        "    stemmed_words = []\n",
        "    for word in words:\n",
        "       stemmed_words.append(stemmer.stem(word))\n",
        "    return stemmed_words\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUkwsbX3keA3",
        "colab_type": "text"
      },
      "source": [
        "# **Remove special characters function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_L1ta0-qkZnr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def remove_special_characters(text):\n",
        "    regex = r'[^a-zA-Z0-9\\s]'\n",
        "    text = re.sub(regex,'',text)\n",
        "    return text\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7fHxHCVkzME",
        "colab_type": "text"
      },
      "source": [
        "# **Calculating the frequency of each word in the document**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMNb--Mdk5ng",
        "colab_type": "text"
      },
      "source": [
        "Here we take the list of words as input and append all the unique words in a new list.\n",
        "\n",
        "The unique words are stored in words_unique list. \n",
        "\n",
        "After finding the unique words, the frequency of the word can be found using count function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcRlTT8ykcLg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def freq(words):\n",
        "    words = [word.lower() for word in words]\n",
        "    dict_freq = {}\n",
        "    words_unique = []\n",
        "    for word in words:\n",
        "       if word not in words_unique:\n",
        "           words_unique.append(word)\n",
        "    for word in words_unique:\n",
        "       dict_freq[word] = words.count(word)\n",
        "    return dict_freq\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5rRIvXolbmP",
        "colab_type": "text"
      },
      "source": [
        "# **POS tagging function**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAfkOorYl5ml",
        "colab_type": "text"
      },
      "source": [
        "This function uses nltk library to pos tag all the words in the text and returns only the nouns and verbs from the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycXYz3dzkl7i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def pos_tagging(text):\n",
        "    pos_tag = nltk.pos_tag(text.split())\n",
        "    pos_tagged_noun_verb = []\n",
        "    for word,tag in pos_tag:\n",
        "        if tag == \"NN\" or tag == \"NNP\" or tag == \"NNS\" or tag == \"VB\" or tag == \"VBD\" or tag == \"VBG\" or tag == \"VBN\" or tag == \"VBP\" or tag == \"VBZ\":\n",
        "             pos_tagged_noun_verb.append(word)\n",
        "    return pos_tagged_noun_verb\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oIqYE4cmWN_",
        "colab_type": "text"
      },
      "source": [
        "# **tf score function**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZevcMTULmXkc",
        "colab_type": "text"
      },
      "source": [
        "This function calculates the tf score of a word. \n",
        "\n",
        "tf is calculated as the number of times the word appears in the sentence upon the total number of words in the sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_y8qH-nkoie",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def tf_score(word,sentence):\n",
        "    freq_sum = 0\n",
        "    word_frequency_in_sentence = 0\n",
        "    len_sentence = len(sentence)\n",
        "    for word_in_sentence in sentence.split():\n",
        "        if word == word_in_sentence:\n",
        "            word_frequency_in_sentence = word_frequency_in_sentence + 1\n",
        "    tf =  word_frequency_in_sentence/ len_sentence\n",
        "    return tf\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5UPPfYSmdw4",
        "colab_type": "text"
      },
      "source": [
        "# **idf score function**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XF1Vf8pmee0",
        "colab_type": "text"
      },
      "source": [
        "This function finds the idf score of the word, by dividing the total number of sentences by number of sentences containing the word and then taking a log10 of that value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gg-9c1Gxkqyy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def idf_score(no_of_sentences,word,sentences):\n",
        "    no_of_sentence_containing_word = 0\n",
        "    for sentence in sentences:\n",
        "        sentence = remove_special_characters(str(sentence))\n",
        "        sentence = re.sub(r'\\d+', '', sentence)\n",
        "        sentence = sentence.split()\n",
        "        sentence = [word for word in sentence if word.lower() not in Stopwords and len(word)>1]\n",
        "        sentence = [word.lower() for word in sentence]\n",
        "        sentence = [wordlemmatizer.lemmatize(word) for word in sentence]\n",
        "        if word in sentence:\n",
        "            no_of_sentence_containing_word = no_of_sentence_containing_word + 1\n",
        "    idf = math.log10(no_of_sentences/no_of_sentence_containing_word)\n",
        "    return idf\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHxJpQSDmj9v",
        "colab_type": "text"
      },
      "source": [
        "# **tfidf score function**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7neakFEjmlWO",
        "colab_type": "text"
      },
      "source": [
        "This function returns the tfidf score, by simply multiplying the tf and idf values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwVXnsH-ksu6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def tf_idf_score(tf,idf):\n",
        "    return tf*idf\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GT1Xu5amDUF",
        "colab_type": "text"
      },
      "source": [
        "# **Word tfidf function**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeZkTYFGmFJb",
        "colab_type": "text"
      },
      "source": [
        "This function calls tf_score, idf_score and tf_idf function.\n",
        "\n",
        "tf_score calculates the tf score, idf_score calculates the idf score and tf_idf_score calculates the tfidf score. It returns the tfidf score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixKEAgjJkutk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def word_tfidf(dict_freq,word,sentences,sentence):\n",
        "    word_tfidf = []\n",
        "    tf = tf_score(word,sentence)\n",
        "    idf = idf_score(len(sentences),word,sentences)\n",
        "    tf_idf = tf_idf_score(tf,idf)\n",
        "    return tf_idf\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JhOr5kYlOnE",
        "colab_type": "text"
      },
      "source": [
        "# **Calculating sentence score**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZ-4FFe9lT9P",
        "colab_type": "text"
      },
      "source": [
        "The score of each sentence can be calculated using sentence_importance function. \n",
        "\n",
        "It involves POS tagging of words in the sentence by pos_tagging function.This function returns only the noun and verb tagged words. \n",
        "\n",
        "The returned words from pos_tagging function are sent to word_tfidf function to calculate the score of that word in the document by calculating its tfidf score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKw71bOMkwb7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def sentence_importance(sentence,dict_freq,sentences):\n",
        "     sentence_score = 0\n",
        "     sentence = remove_special_characters(str(sentence)) \n",
        "     sentence = re.sub(r'\\d+', '', sentence)\n",
        "     pos_tagged_sentence = [] \n",
        "     no_of_sentences = len(sentences)\n",
        "     pos_tagged_sentence = pos_tagging(sentence)\n",
        "     for word in pos_tagged_sentence:\n",
        "          if word.lower() not in Stopwords and word not in Stopwords and len(word)>1: \n",
        "                word = word.lower()\n",
        "                word = wordlemmatizer.lemmatize(word)\n",
        "                sentence_score = sentence_score + word_tfidf(dict_freq,word,sentences,sentence)\n",
        "     return sentence_score\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4_v468FZhH8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = '''The Federal Reserve Bank of New York president, John C. Williams, made clear on Thursday evening that officials viewed the emergency rate cut they approved earlier this week as part of an international push to cushion the economy as the coronavirus threatens global growth.\n",
        "Mr. Williams, one of the Fed’s three key leaders, spoke in New York two days after the Fed slashed borrowing costs by half a point in its first emergency move since the depths of the 2008 financial crisis. The move came shortly after a call between finance ministers and central bankers from the Group of 7, which also includes Britain, Canada, France, Germany, Italy and Japan.\n",
        "“Tuesday’s phone call between G7 finance ministers and central bank governors, the subsequent statement, and policy actions by central banks are clear indications of the close alignment at the international level,” Mr. Williams said in a speech to the Foreign Policy Association.\n",
        "Rate cuts followed in Canada, Asia and the Middle East on Wednesday. The Bank of Japan and European Central Bank — which already have interest rates set below zero — have yet to further cut borrowing costs, but they have pledged to support their economies.\n",
        "Mr. Williams’s statement is significant, in part because global policymakers were criticized for failing to satisfy market expectations for a coordinated rate cut among major economies. Stock prices temporarily rallied after the Fed’s announcement, but quickly sank again.\n",
        "Central banks face challenges in offsetting the economic shock of the coronavirus.\n",
        "Many were already working hard to stoke stronger economic growth, so they have limited room for further action. That makes the kind of carefully orchestrated, lock step rate cut central banks undertook in October 2008 all but impossible.\n",
        "Interest rate cuts can also do little to soften the near-term hit from the virus, which is forcing the closure of offices and worker quarantines and delaying shipments of goods as infections spread across the globe.\n",
        "“It’s up to individual countries, individual fiscal policies and individual central banks to do what they were going to do,” Fed Chair Jerome H. Powell said after the cut, noting that different nations had “different situations.”\n",
        "Mr. Williams reiterated Mr. Powell’s pledge that the Fed would continue monitoring risks in the “weeks and months” ahead. Economists widely expect another quarter-point rate cut at the Fed’s March 18 meeting.\n",
        "The New York Fed president, whose reserve bank is partly responsible for ensuring financial markets are functioning properly, also promised that the Fed stood ready to act as needed to make sure that everything is working smoothly.\n",
        "Since September, when an obscure but crucial corner of money markets experienced unusual volatility, the Fed has been temporarily intervening in the market to keep it calm. The goal is to keep cash flowing in the market for overnight and short-term loans between banks and other financial institutions. The central bank has also been buying short-term government debt.\n",
        "“We remain flexible and ready to make adjustments to our operations as needed to ensure that monetary policy is effectively implemented and transmitted to financial markets and the broader economy,” Mr. Williams said Thursday.'''"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdXuK-3ikAbE",
        "colab_type": "text"
      },
      "source": [
        "# **Text Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FW_6sM1ykJca",
        "colab_type": "text"
      },
      "source": [
        "After reading the contents, remove_special_characters function removes special characters from the text. \n",
        "\n",
        "It is important to remove digits from the document, which can be done using regular expression. \n",
        "\n",
        "After eliminating special character and digits, the individual words can be tokenized and one letter word, stop words can be removed.\n",
        "\n",
        "To avoid any ambiguity in case, we lowercase all the tokenized words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZc_NyoyZk-l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenized_sentence = sent_tokenize(text)\n",
        "\n",
        "text = remove_special_characters(str(text))\n",
        "text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "tokenized_words_with_stopwords = word_tokenize(text)\n",
        "tokenized_words = [word for word in tokenized_words_with_stopwords if word not in Stopwords]\n",
        "tokenized_words = [word for word in tokenized_words if len(word) > 1]\n",
        "tokenized_words = [word.lower() for word in tokenized_words]\n",
        "tokenized_words = lemmatize_words(tokenized_words)\n",
        "\n",
        "word_freq = freq(tokenized_words)\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WArAyiQqnYZo",
        "colab_type": "text"
      },
      "source": [
        "number of sentences are calculated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EOEIO3pZ-Ao",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "3e965272-26ba-4363-be67-7a8e21d31b88"
      },
      "source": [
        "input_user = int(input('Percentage of information to retain(in percent):'))\n",
        "\n",
        "no_of_sentences = int((input_user * len(tokenized_sentence))/100)\n",
        "print(no_of_sentences)\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage of information to retain(in percent):56\n",
            "10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yc2BYo-7m7HS",
        "colab_type": "text"
      },
      "source": [
        "# **Finding most important sentences**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyXrykqdm76k",
        "colab_type": "text"
      },
      "source": [
        "To find the most important sentences, take the individual sentences from tokenized sentences and compute the sentence score. \n",
        "\n",
        "After calculating the scores, the top sentences based on the retention rate provided by the user are included in the summary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YW0U9GdjaDRF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c = 1\n",
        "\n",
        "sentence_with_importance = {}\n",
        "for sent in tokenized_sentence:\n",
        "    sentenceimp = sentence_importance(sent,word_freq,tokenized_sentence)\n",
        "    sentence_with_importance[c] = sentenceimp\n",
        "    c = c+1\n",
        "\n",
        "sentence_with_importance = sorted(sentence_with_importance.items(), key=operator.itemgetter(1),reverse=True)\n",
        "\n",
        "cnt = 0\n",
        "summary = []\n",
        "sentence_no = []\n",
        "\n",
        "for word_prob in sentence_with_importance:\n",
        "    if cnt < no_of_sentences:\n",
        "        sentence_no.append(word_prob[0])\n",
        "        cnt = cnt+1\n",
        "    else:\n",
        "      break\n",
        "\n",
        "sentence_no.sort()\n",
        "cnt = 1\n",
        "for sentence in tokenized_sentence:\n",
        "    if cnt in sentence_no:\n",
        "       summary.append(sentence)\n",
        "    cnt = cnt+1\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DU_Gd3dyaPXc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "d9a679b8-758f-46d0-e482-b7e7b82880f0"
      },
      "source": [
        "summary = \" \".join(summary)\n",
        "print(\"\\n\")\n",
        "print(\"Summary:\")\n",
        "print(summary)\n",
        "outF = open('summary.txt',\"w\")\n",
        "outF.write(summary)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Summary:\n",
            "The Federal Reserve Bank of New York president, John C. Williams, made clear on Thursday evening that officials viewed the emergency rate cut they approved earlier this week as part of an international push to cushion the economy as the coronavirus threatens global growth. Mr. Williams’s statement is significant, in part because global policymakers were criticized for failing to satisfy market expectations for a coordinated rate cut among major economies. Central banks face challenges in offsetting the economic shock of the coronavirus. Many were already working hard to stoke stronger economic growth, so they have limited room for further action. That makes the kind of carefully orchestrated, lock step rate cut central banks undertook in October 2008 all but impossible. Interest rate cuts can also do little to soften the near-term hit from the virus, which is forcing the closure of offices and worker quarantines and delaying shipments of goods as infections spread across the globe. Economists widely expect another quarter-point rate cut at the Fed’s March 18 meeting. The New York Fed president, whose reserve bank is partly responsible for ensuring financial markets are functioning properly, also promised that the Fed stood ready to act as needed to make sure that everything is working smoothly. Since September, when an obscure but crucial corner of money markets experienced unusual volatility, the Fed has been temporarily intervening in the market to keep it calm. The central bank has also been buying short-term government debt.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1554"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    }
  ]
}